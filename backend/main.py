# main.py

import os
import json
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)

# âœï¸ Dá»¯ liá»‡u huáº¥n luyá»‡n máº«u (cÃ³ thá»ƒ thay báº±ng dá»¯ liá»‡u thá»±c táº¿)
samples = [
    {"prompt": "Hi", "response": "Hello! How can I help you today?"},
    {"prompt": "What's your name?", "response": "I'm your AI assistant."},
    {"prompt": "Tell me a joke", "response": "Why did the computer get cold? Because it forgot to close its Windows."},
    {"prompt": "What is Python?", "response": "Python is a powerful, easy-to-learn programming language."},
    {"prompt": "How are you?", "response": "I'm doing great, thank you!"},
    {"prompt": "Who created you?", "response": "I was built by an awesome developer."},
    {"prompt": "What can you do?", "response": "I can chat with you and help answer questions."},
    {"prompt": "Goodbye", "response": "Goodbye! Have a nice day!"}
]

# ğŸ“ Táº¡o thÆ° má»¥c data náº¿u chÆ°a cÃ³
os.makedirs("data", exist_ok=True)
data_path = os.path.join("data", "train.json")

# ğŸ’¾ Ghi file JSON náº¿u chÆ°a tá»“n táº¡i
if not os.path.exists(data_path):
    with open(data_path, "w", encoding="utf-8") as f:
        json.dump(samples, f, ensure_ascii=False, indent=2)
    print("âœ… ÄÃ£ táº¡o file data/train.json")
else:
    print("ğŸ“ ÄÃ£ cÃ³ file data/train.json, sá»­ dá»¥ng láº¡i.")

# ğŸ”„ Load tokenizer vÃ  model (dÃ¹ng distilgpt2 nháº¹, nhanh)
print("ğŸ”„ Äang táº£i tokenizer vÃ  mÃ´ hÃ¬nh distilgpt2...")
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

# Sá»­a lá»—i thiáº¿u pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained("distilgpt2")

# ğŸ“‚ Load vÃ  chuáº©n hÃ³a dá»¯ liá»‡u
print("ğŸ“‚ Äang load dá»¯ liá»‡u...")
dataset = load_dataset("json", data_files={"train": data_path}, split="train")

def tokenize(example):
    text = f"User: {example['prompt']}\nBot: {example['response']}"
    return tokenizer(text, truncation=True, padding="max_length", max_length=128)

tokenized_dataset = dataset.map(tokenize)

# âš™ï¸ ThÃ´ng sá»‘ huáº¥n luyá»‡n
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    logging_dir="./logs",
    save_total_limit=1,
    logging_steps=5,
    save_strategy="no"
)

# ğŸ§± Collator cho causal LM
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# ğŸš€ Huáº¥n luyá»‡n mÃ´ hÃ¬nh
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

print("ğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n...")
trainer.train()
print("âœ… Huáº¥n luyá»‡n xong!")

# ğŸ’¾ LÆ°u mÃ´ hÃ¬nh
save_dir = "trained_model"
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)
print(f"âœ… MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u táº¡i: {save_dir}/")
